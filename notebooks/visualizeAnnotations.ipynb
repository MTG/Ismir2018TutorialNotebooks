{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saraga annotations samples\n",
    "\n",
    "This notebook aims at demonstrating some of the contents of Saraga database annotations: sections, typical phrases, sama and tempo annotations. It is assumed that the notebook downloadAllSARAGAContent.ipynb has been run and data is downloaded in two folders: 'hindustani' and 'carnatic'.\n",
    "\n",
    "The particular file for which we demonstrate the annotations is set in cell 2. To pick another file and simply change the wave file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython\n",
    "\n",
    "from essentia.standard import *\n",
    "from essentia import array\n",
    "\n",
    "fs = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute **one** of the follwing cells to configure the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carnatic samples\n",
    "music_tradition = 'carnatic'\n",
    "file_base = 'Cherthala Ranganatha Sharma - Bhuvini Dasudane'\n",
    "#file_base = 'Akkarai Sisters - Koti Janmani'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindustani samples\n",
    "music_tradition = 'hindustani'\n",
    "file_base = 'Omkar Dadarkar - Bhairavi Dadra'\n",
    "#file_base = 'Ajoy Chakrabarty - Bilaskhani Todi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = os.path.join(music_tradition, file_base)\n",
    "\n",
    "#Annotation files\n",
    "audio_file = '{}.mp3'.format(base_name)\n",
    "metadata_file = '{}.json'.format(base_name)\n",
    "phrase_annot_file = '{}.mphrases-manual.txt'.format(base_name)\n",
    "sama_annot_file = '{}.sama-manual.txt'.format(base_name)\n",
    "bpm_annot_file = '{}.bpm-manual.txt'.format(base_name)\n",
    "section_annot_file = '{}.sections-manual.txt'.format(base_name)\n",
    "tonic_file = '{}.ctonic.txt'.format(base_name)\n",
    "\n",
    "#Reading files(audio and annotations)\n",
    "sound_sig = MonoLoader(filename=audio_file, sampleRate=fs)()\n",
    "with open(metadata_file) as json_data:\n",
    "    metadata = json.load(json_data)\n",
    "phrase_annotations = np.loadtxt(phrase_annot_file, dtype={'names': ('start', 'dummy', 'duration', 'phrase'),'formats': ('f4', 'i4', 'f4', 'S32')})\n",
    "sama_annotations = np.loadtxt(sama_annot_file)\n",
    "tonic = np.loadtxt(tonic_file)\n",
    "bpm_annotations = []\n",
    "with open(bpm_annot_file) as fin:\n",
    "    for line in fin:\n",
    "        # tempo-free sections have '-' label, better to set it to -1 for simplicity of conversion \n",
    "        line = line.replace('-,', '-1,')\n",
    "        bpm_annotations.append(tuple([float(i) for i in line.split(',')]))\n",
    "section_annotations = []\n",
    "with open(section_annot_file) as fin:\n",
    "    for line in fin:\n",
    "        # hindustani uses comma seperator, carnatic tab seperator, this line unifies\n",
    "        line = line.strip().replace('\\t', ',')\n",
    "        pList = line.split(',')\n",
    "        section_annotations.append(tuple([float(pList[0]), float(pList[1]), float(pList[2]), pList[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print short info on the recording\n",
    "#print('Raaga: ',metadata['raaga'][0]['name'],',\\tTaala: ',metadata['taala'][0]['name'])\n",
    "print('MusicBrainz link: https://musicbrainz.org/recording/{}'.format(metadata['mbid']))\n",
    "print('Tonic: {}Hz'.format(tonic))\n",
    "print()\n",
    "\n",
    "print('\\033[1m{:<15}{:<10}{:<10}\\033[0m'.format('Section', 'Start', 'Stop'))\n",
    "for start, _, duration, section in section_annotations:\n",
    "    print('{section:<15}{start:<10}{stop:<10}'.format(section=section, start=round(start, 2), stop=round(start+duration, 2)))\n",
    "\n",
    "print()\n",
    "print('\\033[1m{:<7}{:<7}{:<7}\\033[0m'.format('BPM', 'Start', 'Stop'))\n",
    "for bpm, start, stop in bpm_annotations:\n",
    "    print('{bpm:<7}{start:<7}{stop:<7}'.format(bpm=bpm, start=round(start, 2), stop=round(start+duration, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical phrases\n",
    "Let's extract some phrases available in the phrase level annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dict = collections.defaultdict(list)\n",
    "for start_sec, _, duration_sec, phrase in phrase_annotations:\n",
    "    start_ind = int(fs*start_sec)\n",
    "    stop_ind = start_ind + int(fs*duration_sec)\n",
    "    phrase_dict[phrase].append((start_ind, stop_ind))\n",
    "\n",
    "for phrase, segments in phrase_dict.items():\n",
    "    print(phrase.decode())\n",
    "    for start_ind, stop_ind in segments:\n",
    "        phrase_sig = sound_sig[start_ind:stop_ind]\n",
    "        IPython.display.display(IPython.display.Audio(phrase_sig, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhythmic cycles\n",
    "Sama annotations include begining of cycles. Let's view some cycles, also extract automatically beats and visualize all together.\n",
    "For a simple introduction to tala with examples, refer to: http://compmusic.upf.edu/examples-taala-carnatic\n",
    "\n",
    "To also demonstrate accessing section information, we will be using the cycles in the second section, also comparing annotated tempo and tempo estimated suing the Rhythm Extractor implementation in Essentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting theoretical descriptions of the taala\n",
    "imageUrl = 'http://compmusic.upf.edu/sites/all/themes/litejazz/images/logotoweb.png'#Default image\n",
    "folder_name = 'http://compmusic.upf.edu/system/files/static_files/'\n",
    "if music_tradition == 'carnatic':\n",
    "    print('Taala: %s' % metadata['taala'][0]['name'])\n",
    "    taala_name = metadata['taala'][0]['name']\n",
    "    if taala_name == 'Miśra chāpu':\n",
    "        imageUrl = folder_name + 'mChapu.png'\n",
    "    elif taala_name == 'Ādi':\n",
    "        imageUrl = folder_name + 'Aditaala_illustrated.png'\n",
    "    elif taala_name == 'Rūpaka':\n",
    "        imageUrl = folder_name + 'rupakam_annotations.png'\n",
    "    elif taala_name == 'Khaṇḍa chāpu':\n",
    "        imageUrl = folder_name + 'kChapu_annotations.png'\n",
    "elif music_tradition == 'hindustani':#http://compmusic.upf.edu/examples-taal-hindustani\n",
    "    print('Taal: %s' % metadata['taals'][0]['name'])\n",
    "    taals_name = metadata['taals'][0]['name']\n",
    "    if taals_name == 'ēktāl':\n",
    "        imageUrl = folder_name + 'ektaal_vilambit_maatras.png'\n",
    "    elif taals_name == 'Tīntāl':\n",
    "        imageUrl = folder_name + 'teentaal_matras.png'\n",
    "    elif taals_name == 'Jhaptāl':\n",
    "        imageUrl = folder_name + 'Jhaptaal_matras.png'\n",
    "    elif taals_name == 'Rūpak':\n",
    "        imageUrl = folder_name + 'rupak_maatras.png'\n",
    "    elif taals_name == 'Dādrā':\n",
    "        imageUrl = folder_name + 'dadra.png'\n",
    "    \n",
    "IPython.display.Image(imageUrl, width = 300, height = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section selection is performed in this cell\n",
    "#We will display second section content, id=1\n",
    "selected_section_ind = 1\n",
    "(section_start, dummy, section_duration, section_tag) = section_annotations[selected_section_ind]\n",
    "section_stop=section_start+section_duration\n",
    "(section_bpm, bpm_start, bpm_stop) = bpm_annotations[selected_section_ind]\n",
    "print('Section: {}, annotated bpm: {}'.format(section_tag, section_bpm))\n",
    "print('Location in audio: (start,stop in secs): {}, {}'.format(round(section_start, 2), round(section_stop, 2)))\n",
    "\n",
    "#Extracting corresponding sama annnotations in the section\n",
    "sama_annotations_seg = sama_annotations[np.logical_and(sama_annotations >= section_start, sama_annotations < section_stop)]\n",
    "start_seg = sama_annotations_seg[0]\n",
    "stop_seg = sama_annotations_seg[-1]\n",
    "sama_annotations_seg = sama_annotations_seg - start_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing annotations and estimation\n",
    "Below, we use rhythm extractor in Essentia to estimate beats for the selected section. \n",
    "Sama annotations and beats are sonified together with the original audio.\n",
    "We also plot annotated tempo versus estimated tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beat detection using Essentia\n",
    "\n",
    "#Plotting\n",
    "sound_sig_seg = sound_sig[int(start_seg * fs):int(stop_seg * fs)]\n",
    "t = np.arange(sound_sig_seg.size)/float(fs)\n",
    "zero_array = t * 0 #used only for plotting purposes\n",
    "f, axarr = plt.subplots(3,1,figsize=(13, 6))\n",
    "axarr[0].plot(t, sound_sig_seg);\n",
    "axarr[0].set_title(audio_file);axarr[0].axis('off')\n",
    "axarr[0].vlines(sama_annotations_seg, -1, 1.2, color = 'b')\n",
    "\n",
    "rhythm_extractor = RhythmExtractor2013(method = \"multifeature\")\n",
    "bpm, beats, beats_confidence, _, beats_intervals = rhythm_extractor(sound_sig_seg)\n",
    "onsetMarker = AudioOnsetsMarker(onsets = beats, type = 'noise')\n",
    "samaMarker = AudioOnsetsMarker(onsets = array(sama_annotations_seg), type = 'beep')\n",
    "marked_sound_sig = samaMarker(onsetMarker(sound_sig_seg))\n",
    "\n",
    "axarr[1].plot(t, zero_array);\n",
    "axarr[1].set_title('Beats estimated');\n",
    "axarr[1].axis('off')\n",
    "axarr[1].vlines(beats, -1, 1, color='r')\n",
    "axarr[1].vlines(sama_annotations_seg, -1, 1, color='b')\n",
    "\n",
    "\n",
    "axarr[2].plot(60/beats_intervals);\n",
    "axarr[2].set_title('Estimated BPM');\n",
    "axarr[2].hlines([section_bpm/2, section_bpm, section_bpm * 2], 0, len(beats_intervals), color = 'k', label = 'Annotated BPM * (0.5, 1, 2)')\n",
    "axarr[2].legend()\n",
    "axarr[2].set_ylim(0, section_bpm*2.5)\n",
    "\n",
    "print('Audio with sonified sama annotations(beeps) and estimated beats(clicks)')\n",
    "IPython.display.Audio(marked_sound_sig, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations of the IEMP data\n",
    "\n",
    "In this part, we present a sample from the Interpersonal Entrainment in Music Performance data by Clayton, Eerola, Jakubowski, Tarsitani and Leante available on https://osf.io/ks325/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = 'TABLA'\n",
    "#instrument = 'SAROD'\n",
    "\n",
    "if instrument == 'TABLA':\n",
    "    wav_file = '../data/IEMP_north_indian_raga/Sample/Media/NIR_PrB_Jhinjhoti_2Gats_Tabla_sample.mp3'\n",
    "    section_annot_file = '../data/IEMP_north_indian_raga/Sample/Annotations/NIR_PrB_Jhinjhoti_2Gats_Annotation_Sample.csv'\n",
    "    onset_annot_file = '../data/IEMP_north_indian_raga/Sample/Annotations/NIR_PrB_Jhinjhoti_2Gats_Onsets_Raw_Tabla_Sample.csv'\n",
    "elif instrument == 'SAROD':\n",
    "    wav_file = '../data/IEMP_north_indian_raga/Sample/Media/NIR_PrB_Jhinjhoti_2Gats_Sarod_sample.mp3'\n",
    "    section_annot_file = '../data/IEMP_north_indian_raga/Sample/Annotations/NIR_PrB_Jhinjhoti_2Gats_Annotation_Sample.csv'\n",
    "    onset_annot_file = '../data/IEMP_north_indian_raga/Sample/Annotations/NIR_PrB_Jhinjhoti_2Gats_Onsets_Raw_Sarod_Sample.csv'\n",
    "\n",
    "\n",
    "#Reading files(audio and annotations)\n",
    "sound_sig = MonoLoader(filename=wav_file, sampleRate=fs)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading section annotations\n",
    "with open(section_annot_file) as fin:\n",
    "    for line in fin:\n",
    "        if instrument in line:\n",
    "            pList = line.split(',')\n",
    "            start_seg = float(pList[2])\n",
    "            stop_seg = float(pList[3])\n",
    "\n",
    "#for zooming purposes, let's set analysis range to the 40:50 sec within the specific range\n",
    "start_seg = start_seg + 40\n",
    "stop_seg = start_seg + 10\n",
    "\n",
    "#Reading onsets within the tabla section\n",
    "onset_annots = []\n",
    "with open(onset_annot_file) as fin:\n",
    "    for line in fin:\n",
    "        if 'Time' not in line: #skip row containing column labels\n",
    "            onset = float(line.split(',')[0])\n",
    "            #keeping onsets within tabla section \n",
    "            if onset > start_seg and onset < stop_seg:\n",
    "                onset_annots.append(onset-start_seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell, we demonstrate computation of an onset strength function\n",
    "# Essentia includes many onset strength functions and onset detection methods\n",
    "# Please refer to the documentation and try out on your own different options for analysis\n",
    "\n",
    "sound_sig_seg = sound_sig[int(start_seg * fs):int(stop_seg * fs)]\n",
    "t = np.arange(sound_sig_seg.size)/float(fs)\n",
    "\n",
    "#sonifying onsets\n",
    "onsetMarker = AudioOnsetsMarker(onsets=array(onset_annots), type='noise')\n",
    "marked_sound_sig = onsetMarker(sound_sig_seg)\n",
    "\n",
    "# Computing the onset strength function: high-frequency content\n",
    "w = Windowing(type='hann')\n",
    "fft = FFT() # this gives us a complex FFT\n",
    "c2p = CartesianToPolar() # and this transforms it into a pair (magnitude, phase)\n",
    "\n",
    "hfc = []\n",
    "for frame in FrameGenerator(sound_sig_seg, frameSize = 1024, hopSize = 512):\n",
    "    mag, phase, = c2p(fft(w(frame)))\n",
    "    hfc.append(np.dot(np.power(mag, 2), np.arange(mag.size)))\n",
    "hfc=np.array(hfc)\n",
    "\n",
    "#Plots\n",
    "f, axarr = plt.subplots(2,1,figsize=(13, 6))\n",
    "axarr[0].plot(t, sound_sig_seg);\n",
    "axarr[0].set_title(wav_file);axarr[0].axis('off')\n",
    "axarr[0].vlines(onset_annots, -1, 1.2, color = 'b')\n",
    "\n",
    "#Plotting the high-frequency content function\n",
    "axarr[1].plot(hfc,label='High-freq.content')\n",
    "axarr[1].axis('off')\n",
    "axarr[1].legend(loc=1)\n",
    "axarr[1].axis('off')\n",
    "\n",
    "#Little exercise for you: implement a method to detect the onset locations from the high-frequency content function\n",
    "\n",
    "IPython.display.Audio(marked_sound_sig, rate=fs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
