{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Saraga database\n",
    "\n",
    "This notebook demonstrates the use of the Dunya api for downloading Saraga dataset files, which includes Audio (mp3s under a Creative Commons license), metadata, automatically extracted features, and manual annotation files .\n",
    "\n",
    "The full Saraga dataset is also available for direct download on Zenodo: https://doi.org/10.5281/zenodo.1256126\n",
    "\n",
    "The Saraga dataset is composed of two collections:\n",
    "- [Hindustani collection](https://musicbrainz.org/collection/6adc54c6-6605-4e57-8230-b85f1de5be2b)\n",
    "- [Carnatic collection](https://musicbrainz.org/collection/a163c8f2-b75f-4655-86be-1504ea2944c2) \n",
    "\n",
    "This notebook creates two subfolders and saves all data in these folders. Each annotation is saved in a separate text file. \n",
    "\n",
    "To be able to download sounds from Dunya, you need to have a user and obtain an API authentication key (token). Please create a user: https://dunya.compmusic.upf.edu/social/register/ \n",
    "In order to get your API token you have to log in to dunya and then go to your profile where you will find your token. \n",
    "\n",
    "For example visualisations of the annotations of this data, refer to the 'visualizeAnnotations' notebook  \n",
    "\n",
    "Authors: Sankalp Gulati, Baris Bozkurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your token here from https://dunya.compmusic.upf.edu/user/profile/\n",
    "token = '...yourAPITokenGoesHere...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json, os, sys\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import compmusic\n",
    "\n",
    "from compmusic import dunya as dn\n",
    "from compmusic.dunya import hindustani as hi\n",
    "from compmusic.dunya import carnatic as ca\n",
    "from compmusic.dunya import docserver as ds\n",
    "from compmusic import musicbrainz\n",
    "\n",
    "dn.set_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features list\n",
    "features_dunya_all = [{'type': 'pitch', 'subtype': 'pitch', 'extension': '.pitch', 'version': 'noguessunv'},\n",
    "                         {'type': 'ctonic', 'subtype': 'tonic', 'extension': '.tonic', 'version': '0.3'},\n",
    "                         {'type': 'sama-manual', 'subtype': None, 'extension': '.sama', 'version': None},\n",
    "                         {'type': 'sections-manual', 'subtype': None, 'extension': '.sections', 'version': None},\n",
    "                         {'type': 'tempo-manual', 'subtype': None, 'extension': '.tempo', 'version': None},\n",
    "                         {'type': 'pitch-vocal', 'subtype': None, 'extension': '.mpitch', 'version': None},\n",
    "                         {'type': 'mphrases-manual', 'subtype': None, 'extension': '.mphrases', 'version': None},\n",
    "                         {'type': 'sections-manual-p', 'subtype': None, 'extension': '.sections_p', 'version': None},\n",
    "                         {'type': 'bpm-manual', 'subtype': None, 'extension': '.bpm', 'version': None}\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for accessing files, computing statistics and writing/saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatsDunyaCorpus():\n",
    "    \"\"\"\n",
    "    Compute and save statistics for the Hindustani and Carnatic collections.\n",
    "    \n",
    "    Outputs:\n",
    "        A Pickle to 'stats_{collection}_cc.pkl' of the MusicBrainz IDs that appear in the collection\n",
    "        A text file to 'stats_{collection}_cc.txt' showing summary counts of items that appear in the collection\n",
    "    \"\"\"\n",
    "\n",
    "    carnatic_stats = get_stats_carnatic(DUNYA_COLLECTIONS['carnatic'])\n",
    "    output_file = 'stats_carnatic_cc.pkl'\n",
    "    output_file_pretty = 'stats_carnatic_cc.txt'\n",
    "    save_stats(carnatic_stats, output_file, output_file_pretty)\n",
    "    \n",
    "    hindustani_stats = get_stats_hindustani(DUNYA_COLLECTIONS['hindustani'])\n",
    "    output_file = 'stats_hindustani_cc.pkl'\n",
    "    output_file_pretty = 'stats_hindustani_cc.txt'\n",
    "    save_stats(hindustani_stats, output_file, output_file_pretty)\n",
    "\n",
    "\n",
    "def get_stats_hindustani(dunya_collections=None):\n",
    "    \"\"\"Get information about hindustani recordings and return a summary of attributes.\n",
    "    For the following attributes:\n",
    "        release\n",
    "        works\n",
    "        raags\n",
    "        taals\n",
    "        forms\n",
    "        layas\n",
    "        album artists\n",
    "        artists (musicians)\n",
    "    generate a list of identifiers for these attributes (mbid or uuid [raag, taal, laya] or name [form])\n",
    "    present in the collection\n",
    "    \n",
    "    Args:\n",
    "        dunya_collections: a list of MusicBrainz/Dunya Collection IDs to restrict the Dunya API to\n",
    "    \"\"\"\n",
    "    \n",
    "    hi.set_collections(dunya_collections)\n",
    "    recordings = hi.get_recordings()\n",
    "\n",
    "    stats = collections.defaultdict(list)\n",
    "    for r in recordings:\n",
    "        mbid = r['mbid']\n",
    "\n",
    "        try:\n",
    "            rec_info = hi.get_recording(mbid)\n",
    "\n",
    "            stats['release'].append([r['mbid'] for r in rec_info.get('release', [])])\n",
    "            stats['works'].append([w['mbid'] for w in rec_info.get('works', [])])\n",
    "            stats['raags'].append([r['uuid'] for r in rec_info.get('raags', [])])\n",
    "            stats['taals'].append([t['uuid'] for t in rec_info.get('taals', [])])\n",
    "            stats['forms'].append([f['name'] for f in rec_info.get('forms', [])])\n",
    "            stats['layas'].append([l['uuid'] for l in rec_info.get('layas', [])])\n",
    "            stats['album_artists'].append([a['mbid'] for a in rec_info.get('album_artists', [])])\n",
    "            stats['artists'].append([a['artist']['mbid'] for a in rec_info.get('artists', [])])\n",
    "            stats['length'].append(rec_info.get('length'))\n",
    "        except:\n",
    "            failure+=1\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(\"Failed to fetch info for recording %s\" % mbid) \n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "    \n",
    "    # Filter empty lists from the stats\n",
    "    for k, vals in stats.items():\n",
    "        stats[k] = [v for v in vals if v]\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def get_stats_carnatic(dunya_collections=None):\n",
    "    \"\"\"Get information about carnatic recordings and return a summary of attributes.\n",
    "    For the following attributes:\n",
    "        concert\n",
    "        work\n",
    "        raaga\n",
    "        taala\n",
    "        form\n",
    "        album artists\n",
    "        artists (musicians)\n",
    "    generate a list of identifiers for these attributes (mbid or uuid [raaga, taala] or name [form])\n",
    "    present in the collection\n",
    "    \n",
    "    Args:\n",
    "        dunya_collections: a list of MusicBrainz/Dunya Collection IDs to restrict the Dunya API to\n",
    "    \"\"\"\n",
    "    ca.set_collections(dunya_collections)\n",
    "    recordings = ca.get_recordings()\n",
    "    \n",
    "    stats = collections.defaultdict(list)\n",
    "    for r in recordings:\n",
    "        mbid = r['mbid']\n",
    "\n",
    "        try:\n",
    "            rec_info = ca.get_recording(mbid)\n",
    "\n",
    "            stats['concert'].append([c['mbid'] for c in rec_info.get('concert', [])])\n",
    "            stats['work'].append([w['mbid'] for w in rec_info.get('work', [])])\n",
    "            stats['raaga'].append([r['uuid'] for r in rec_info.get('raaga', [])])\n",
    "            stats['taala'].append([t['uuid'] for t in rec_info.get('taala', [])])\n",
    "            stats['form'].append([f['name'] for f in rec_info.get('form', [])])\n",
    "            stats['album_artists'].append([a['mbid'] for a in rec_info.get('album_artists', [])])\n",
    "            stats['artists'].append([a['artist']['mbid'] for a in rec_info.get('artists', [])])\n",
    "            stats['length'].append(rec_info.get('length'))\n",
    "        except dn.HTTPError:\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(\"Failed to fetch info for recording %s\" % mbid) \n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        \n",
    "    # Filter empty lists from the stats\n",
    "    for k, vals in stats.items():\n",
    "        stats[k] = [v for v in vals if v]\n",
    "    \n",
    "    return stats\n",
    "\n",
    "    \n",
    "def save_stats(stats, stats_file, summary_file):\n",
    "    \"\"\"Write statistics to file\n",
    "    Args:\n",
    "        stats (dict): the statistics to write\n",
    "        stats_file (str): file path to write the statistics summary\n",
    "        summary_file (str): file path to write a readable statistics summary\n",
    "    \"\"\"\n",
    "    \n",
    "    output_stats = {}\n",
    "    for k, v in stats.items():\n",
    "        if k == 'length':\n",
    "            output_stats[k] = {'total_length': np.sum(v), 'total_recs': len(v)}\n",
    "        else:\n",
    "            output_stats[k] = {'total_unique': len(np.unique(sum(v, []))), 'unique_elems': np.unique(sum(v, [])).tolist(), 'total_rels': len(sum(v, [])), 'total_recs': len(v)}\n",
    "    pickle.dump(output_stats, codecs.open(stats_file, 'wb'))\n",
    "    \n",
    "    \n",
    "    with codecs.open(summary_file, 'w') as fp:\n",
    "        for key1, val in output_stats.items():\n",
    "            fp.write('------------ %s ------------\\n'%str(key1))\n",
    "            if key1 == 'length':\n",
    "                for key2, val2 in val.items():\n",
    "                    fp.write('%s\\t%f\\n'%(str(key2), float(val2)/(1000.0*3600.0)))\n",
    "            else:\n",
    "                for key2, val2 in val.items():\n",
    "                    if key2 == 'unique_elems':\n",
    "                        fp.write('%s\\t%d\\n'%(str(key2), len(val2)))\n",
    "                    else:\n",
    "                        fp.write('%s\\t%d\\n'%(str(key2), val2))\n",
    "            fp.write('\\n')\n",
    "\n",
    "def saveSections(content, output_file):\n",
    "    \"\"\"\n",
    "    This function saves the content(section annotations) into a file in a structured manner\n",
    "    Annotations are already stored nicely but due to differences in the delimiters of Hindustani and Carnatic\n",
    "    we needed this function\n",
    "    \n",
    "    Args:\n",
    "        content (str): data read from dunya api\n",
    "        output_file (str): file path for output file\n",
    "    Outputs:\n",
    "        Saves statistics to a text file\n",
    "    \"\"\"\n",
    "    \n",
    "    # detecting delimiter automatically\n",
    "    snf = csv.Sniffer()\n",
    "    delimiter = snf.sniff(content).delimiter\n",
    "    rows = [k.split(delimiter) for k in content.split('\\n') if k != '']\n",
    "    csv.writer(output_file, rows, delimiter = '\\t')\n",
    "\n",
    "\n",
    "def download_files_for_collection(collection_name, collection_ids, features, numFiles=5):\n",
    "    \"\"\"Download all files of a collection\n",
    "    Args:\n",
    "        collection (dict): dictionary containig name and id of the collection\n",
    "        features (list of dicts): feature types\n",
    "        numFiles (int): the maximum number of files to download\n",
    "    Returns:\n",
    "        A dictionary counting how many files for each feature was unable to be downloaded\n",
    "    Outputs:\n",
    "        Saves mp3 and annotation files of the collection\n",
    "    \"\"\"\n",
    "    dataDir = collection_name\n",
    "    os.makedirs(dataDir, exist_ok=True)\n",
    "\n",
    "    if collection_name == 'hindustani':\n",
    "        tradition = hi\n",
    "    elif collection_name == 'carnatic':\n",
    "        tradition = ca\n",
    "    \n",
    "    tradition.set_collections(collection_ids)\n",
    "    recs = tradition.get_recordings()\n",
    "    \n",
    "    numFiles = min(numFiles, len(recs))\n",
    "    \n",
    "    print('Number of files in collection {}: {}'.format(collection_name, len(recs)))\n",
    "    print('...will download {} files'.format(numFiles))\n",
    "    \n",
    "    # Creating data structure for keeping list of missing files\n",
    "    missingData = collections.Counter()\n",
    "    for feature in features:\n",
    "        missingData[feature['type']] = 0\n",
    "    \n",
    "    # Downloading data\n",
    "    for i, recording in enumerate(recs[:numFiles], 1):\n",
    "        mbid = recording['mbid']\n",
    "        print('{}/{}: {}'.format(i, len(recs), mbid))\n",
    "        mp3_filename = tradition.download_mp3(mbid, dataDir)\n",
    "        json_file = mp3_filename.replace('.mp3', '.json')\n",
    "        with open(os.path.join(dataDir, json_file), 'w') as outfile:\n",
    "            json.dump(tradition.get_recording(mbid), outfile)\n",
    "        \n",
    "        print(mp3_filename)\n",
    "\n",
    "        for feature in features:\n",
    "            try:\n",
    "                content = ds.file_for_document(mbid, feature['type'], feature['subtype'], version=feature['version'])\n",
    "                \n",
    "                out_file = os.path.join(dataDir, mp3_filename.replace('.mp3','.{}.txt'.format(feature['type'])))\n",
    "                if feature['type'] == 'pitch':\n",
    "                    content = json.loads(content.decode())\n",
    "                    content = np.array(content)\n",
    "                    np.savetxt(out_file, content, fmt='%.7f', delimiter='\\t')\n",
    "                #elif feature['type'] == 'sections-manual' or feature['type'] == 'sections-manual-p':\n",
    "                #    saveSections(content.decode(), out_file)\n",
    "                else:\n",
    "                    with open(out_file, 'w') as fp:\n",
    "                        fp.write(content.decode())\n",
    "            except dn.HTTPError:\n",
    "                #print('Does not have ',feature['type'],' content for :',mbid)\n",
    "                missingData[feature['type']] += 1\n",
    "    \n",
    "    print('Collection download finished.')\n",
    "    print('----------------------------------------------------------')\n",
    "    return dict(missingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting collections to be downloaded\n",
    "Collections are specified with a name and musicBrainz id. [All CompMusic collections are listed here](https://musicbrainz.org/user/compmusic/collections)\n",
    "\n",
    "ID refers to the last part of a musicBrainz link for the collection such as\n",
    "https://musicbrainz.org/collection/a163c8f2-b75f-4655-86be-1504ea2944c2 for the Carnatic collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUNYA_COLLECTIONS = {'hindustani': ['6adc54c6-6605-4e57-8230-b85f1de5be2b'],\n",
    "                     'carnatic': ['a163c8f2-b75f-4655-86be-1504ea2944c2']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling functions to produce statistics and download data\n",
    "\n",
    "missingDatas = {}\n",
    "NUM_FILES = 5 #set to 200 if you like to download all data (CAUTION: 8Gb)\n",
    "\n",
    "print('Starting process: {}'.format(datetime.datetime.now()))\n",
    "print('Collecting statistics of carnatic collection')\n",
    "carnatic_stats = get_stats_carnatic(DUNYA_COLLECTIONS['carnatic'])\n",
    "output_file = 'stats_carnatic_cc.pkl'\n",
    "output_file_pretty = 'stats_carnatic_cc.txt'\n",
    "save_stats(carnatic_stats, output_file, output_file_pretty)\n",
    "\n",
    "print('Downloading files ... ')\n",
    "missingData = download_files_for_collection('carnatic', DUNYA_COLLECTIONS['carnatic'], features_dunya_all, NUM_FILES)\n",
    "missingDatas['carnatic'] = missingData\n",
    "\n",
    "print('...Done')\n",
    "print('Collecting statistics of hindustani collection')\n",
    "\n",
    "hindustani_stats = get_stats_hindustani(DUNYA_COLLECTIONS['hindustani'])\n",
    "output_file = 'stats_hindustani_cc.pkl'\n",
    "output_file_pretty = 'stats_hindustani_cc.txt'\n",
    "save_stats(hindustani_stats, output_file, output_file_pretty)\n",
    "\n",
    "print('Downloading files ... ')\n",
    "missingData = download_files_for_collection('hindustani', DUNYA_COLLECTIONS['hindustani'], features_dunya_all, NUM_FILES)\n",
    "missingDatas['hindustani'] = missingData\n",
    "print('...Done')\n",
    "\n",
    "pickle.dump(missingDatas, codecs.open('missingData.pkl', 'wb'))\n",
    "print('Missing data list stored in missingData.pkl')\n",
    "\n",
    "print('Finished! {}'.format(datetime.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
